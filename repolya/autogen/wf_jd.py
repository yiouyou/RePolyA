from repolya._const import WORKSPACE_AUTOGEN, AUTOGEN_JD
from repolya._log import logger_yj

from repolya.autogen.organizer import (
    Organizer,
    ConversationResult,
)

from langchain.chat_models import ChatOpenAI
from langchain.prompts import (
    PromptTemplate,
    ChatPromptTemplate,
)
from langchain.schema import StrOutputParser
from langchain.callbacks import get_openai_callback
from langchain.document_loaders import WebBaseLoader
from langchain.chat_models import ChatOpenAI
from langchain.chains import RetrievalQAWithSourcesChain

from repolya.toolset.tool_langchain import (
    bing,
    ddg,
    google,
)
from repolya.toolset.util import calc_token_cost
from repolya.rag.digest_dir import (
    calculate_md5,
    dir_to_faiss_openai,
)
from repolya.rag.doc_loader import clean_txt
from repolya.rag.digest_urls import urls_to_faiss
from repolya.rag.vdb_faiss import get_faiss_OpenAI
from repolya.rag.qa_chain import (
    qa_vdb_multi_query,
    qa_with_context_as_go,
)

from repolya.autogen.workflow import (
    create_rag_task_list_zh,
    search_faiss_openai,
)

import shutil
import re
import os


def clean_filename(text, max_length=10):
    # ç§»é™¤éæ³•æ–‡ä»¶åå­—ç¬¦ï¼ˆä¾‹å¦‚: \ / : * ? " < > |ï¼‰
    _clean = re.sub(r'[\\/*?:"<>|]', '', text)
    # æ›¿æ¢æ“ä½œç³»ç»Ÿæ•æ„Ÿçš„å­—ç¬¦
    _clean = _clean.replace(' ', '_')  # æ›¿æ¢ç©ºæ ¼ä¸ºä¸‹åˆ’çº¿
    # å–å‰ max_length ä¸ªå­—ç¬¦ä½œä¸ºæ–‡ä»¶å
    return _clean[:max_length]


def search_all(_query):
    _all = []
    # _all.extend(bing(_query))
    _all.extend(ddg(_query))
    # _all.extend(google(_query))
    return _all


def print_search_all(_all):
    _str = []
    for i in _all:
        _str.append(f"{i['link']}\n{i['title']}")
        # _str.append(f"{i['link']}\n{i['title']}\n{i['snippet']}")
    return "\n" + "\n".join(_str)


def fetch_all_link(_all, _event_dir):
    _txt_fp = []
    _all_link = [i['link'] for i in _all]
    _all_title = [i['title'] for i in _all]
    # print(_all_link)
    loader = WebBaseLoader()
    _re = loader.scrape_all(_all_link)
    for i in range(len(_re)):
        _fn = clean_filename(_all_title[i])
        _fp = os.path.join(_event_dir, f"{_fn}.txt")
        with open(_fp, "w") as wf:
            # _txt = _re[i].get_text()
            _txt = _re[i].get_text()
            wf.write(_txt)
        logger_yj.info(f"{_all_link[i]} -> {_fn}.txt")
        _txt_fp.append(_fp)
    return _txt_fp


def handle_fetch(_event_dir, _db_name, _clean_txt_dir):
    logger_yj.info(f"generate faiss_openaiï¼šå¼€å§‹")
    dir_to_faiss_openai(_event_dir, _db_name, _clean_txt_dir)
    logger_yj.info(f"generate faiss_openaiï¼š{_db_name}")
    logger_yj.info(f"generate faiss_openaiï¼šå®Œæˆ")


def task_with_context_template(_task, _context, template):
    llm = ChatOpenAI(temperature=0, model_name="gpt-3.5-turbo")
    prompt = PromptTemplate.from_template(template)
    with get_openai_callback() as cb:
        chain = prompt | llm | StrOutputParser()
        _ans = chain.invoke({"_task": _task, "_context": _context})
        _token_cost = f"Tokens: {cb.total_tokens} = (Prompt {cb.prompt_tokens} + Completion {cb.completion_tokens}) Cost: ${format(cb.total_cost, '.5f')}"
    return [_ans, _token_cost]


def generate_search_query_for_event(_event: str, _event_name: str) -> list[str]:
    _event_dir = str(AUTOGEN_JD / _event_name)
    _query = []
    if not os.path.exists(_event_dir):
        logger_yj.info("generate_search_query_for_eventï¼šå¼€å§‹")
        _task = "è¯·ç”Ÿæˆå…³äº'{_context}'çš„è¯¦ç»†ä¿¡æ¯æŸ¥è¯¢é—®é¢˜åˆ—è¡¨ï¼ˆä¸€ä¸ªé—®é¢˜ä¸€è¡Œï¼‰ã€‚ç¡®ä¿åªåˆ—å‡ºå…·ä½“çš„é—®é¢˜ï¼Œè€Œä¸åŒ…æ‹¬ä»»ä½•ç« èŠ‚æ ‡é¢˜æˆ–ç¼–å·ã€‚"
        _ans, _tc = task_with_context_template(_task, _event, _gsqfe)
        # logger_yj.info(f"\n{_ans}")
        _extracted = re.findall(r'\s+-\s+(.*)\n', _ans)
        for i in _extracted:
            logger_yj.info(i)
        logger_yj.info(_tc)
        _query = _extracted
        logger_yj.info("generate_search_query_for_eventï¼šå®Œæˆ")
    else:
        logger_yj.info(f"'{_event_name}'ä¸“é¢˜å·²å­˜åœ¨ï¼Œæ— éœ€ generate_search_query_for_event")
    return _query


def generate_vdb_for_search_query(_query: list[str], _event_name: str):
    _event_dir = str(AUTOGEN_JD / _event_name)
    if not os.path.exists(_event_dir):
        os.makedirs(_event_dir)
        _db_name = str(AUTOGEN_JD / _event_dir / f"yj_rag_openai")
        _clean_txt_dir = str(AUTOGEN_JD / _event_dir / f"yj_rag_clean_txt")
        logger_yj.info("generate_vdb_for_search_queryï¼šå¼€å§‹")
        # ### search, fetch, vdb
        # for i in _query:
        #     i_all = search_all(i)
        #     i_psa = print_search_all(i_all)
        #     logger_yj.info(f"'{i}'")
        #     logger_yj.info(i_psa)
        #     fetch_all_link(i_all, _event_dir)
        # handle_fetch(_event_dir, _db_name, _clean_txt_dir)
        ### search, load, vdb
        _all = []
        for i in _query:
            i_all = search_all(i)
            _all.extend(i_all)
        _psa = print_search_all(_all)
        logger_yj.info(_psa)
        _all_link = [i['link'] for i in _all]
        _urls = list(set(_all_link))
        urls_to_faiss(_urls, _db_name, _clean_txt_dir)
        logger_yj.info("generate_vdb_for_search_queryï¼šå®Œæˆ")
    else:
        logger_yj.info(f"'{_event_name}'ä¸“é¢˜å·²å­˜åœ¨ï¼Œæ— éœ€ generate_vdb_for_search_query")
        # shutil.rmtree(_event_dir)
        # os.makedirs(_event_dir)


def ask_vdb_with_source(_ask, _vdb):
    llm = ChatOpenAI(temperature=0, model_name="gpt-3.5-turbo-16k")
    chain = RetrievalQAWithSourcesChain.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=_vdb.as_retriever(),
    )
    with get_openai_callback() as cb:
        _res = chain({"question": _ask}, return_only_outputs=True)
        _token_cost = f"Tokens: {cb.total_tokens} = (Prompt {cb.prompt_tokens} + Completion {cb.completion_tokens}) Cost: ${format(cb.total_cost, '.5f')}"
        logger_yj.info(_token_cost)
    return _res['answer'], _res['sources'], _token_cost


def generate_event_context(_event: str, _event_name: str) -> str:
    _db_name = str(AUTOGEN_JD / _event_name / f"yj_rag_openai")
    _ask_vdb = str(AUTOGEN_JD / _event_name / 'ask_vdb.txt')
    if os.path.exists(_ask_vdb):
        os.remove(_ask_vdb)
    logger_yj.info("generate_event_contextï¼šå¼€å§‹")
    _task = "è¯·ç”Ÿæˆå…³äº'{_context}'çš„è¯¦ç»†ä¿¡æ¯æŸ¥è¯¢é—®é¢˜åˆ—è¡¨ï¼ˆä¸€ä¸ªé—®é¢˜ä¸€è¡Œï¼‰ã€‚ç¡®ä¿åªåˆ—å‡ºå…·ä½“çš„é—®é¢˜ï¼Œè€Œä¸åŒ…æ‹¬ä»»ä½•ç« èŠ‚æ ‡é¢˜æˆ–ç¼–å·ã€‚"
    _ans, _tc = task_with_context_template(_task, _event, _gec_ask)
    # logger_yj.info(f"\n{_ans}")
    _extracted = re.findall(r'\s+-\s+(.*)\n', _ans)
    for i in _extracted:
        logger_yj.info(i)
    logger_yj.info(_tc)
    ### ask_vdb
    logger_yj.info("ask_vdbï¼šå¼€å§‹")
    _vdb = get_faiss_OpenAI(_db_name)
    _qas = []
    _tc = []
    for i in _extracted:
        _ask = i + "å¦‚æœæ‰¾ä¸åˆ°ç¡®åˆ‡ç­”æ¡ˆï¼Œè¯·å›ç­”'æ— 'ã€‚ç”¨ç®€æ´ä¸­æ–‡å›ç­”ã€‚"
        ### with source
        # i_ans, i_source, i_token_cost = ask_vdb_with_source(_ask, _vdb)
        # i_qas = f"Q: {i}\nA: {i_ans.strip()}\nSource: {i_source}"
        ### multi query
        # i_ans, i_step, i_token_cost = qa_vdb_multi_query(_ask, _vdb, 'stuff')
        ### autogen
        _task_list, c_token_cost = create_rag_task_list_zh(i)
        _context, s_token_cost = search_faiss_openai(_task_list, _vdb)
        i_ans, i_token_cost = qa_with_context_as_go(_ask, _context)
        ###
        i_ans = clean_txt(i_ans)
        i_ans = i_ans.strip()
        if i_ans != '':
            i_qas = f"Q: {i}\nA: {i_ans}"
            logger_yj.info(i_qas)
            _qas.append(i_qas)
            with open(_ask_vdb, "w") as f:
                f.write("\n\n".join(_qas))
        _tc.append(c_token_cost)
        _tc.append(s_token_cost)
        _tc.append(i_token_cost)
    _token_cost = calc_token_cost(_tc)
    logger_yj.info(_token_cost)
    logger_yj.info("ask_vdbï¼šå®Œæˆ")
    ### 
    _context = "ã€_contextã€‘"
    logger_yj.info("generate_event_contextï¼šå®Œæˆ")
    return _context


def generate_event_plan(_event: str, _event_dir: str, _context:str) -> str:
    _db_name = str(AUTOGEN_JD / _event_dir / f"yj_rag_openai")
    logger_yj.info("generate_event_planï¼šå¼€å§‹")
    _plan = f"{_context}\n\nã€planã€‘"
    logger_yj.info("generate_event_planï¼šå®Œæˆ")
    return _plan


_gsqfe = """ä½ çš„ä»»åŠ¡æ˜¯ä½¿ç”¨æœç´¢å¼•æ“ï¼ˆä¾‹å¦‚è°·æ­Œï¼‰æ¥æœé›†å…³äºç‰¹å®šè‡ªç„¶ç¾å®³äº‹ä»¶çš„å…¨é¢ä¿¡æ¯ã€‚ä½ éœ€è¦å¯»æ‰¾çš„ä¿¡æ¯åŒ…æ‹¬äº‹ä»¶çš„åŸºæœ¬æ¦‚å†µã€å¤„ç½®è¿‡ç¨‹ã€å†›æ°‘åä½œã€å½±å“è¯„ä¼°ä»¥åŠåæ€å’Œå¯ç¤ºã€‚è¯·æŒ‰ç…§ä»¥ä¸‹æŒ‡å—ç”Ÿæˆè¯¦å°½çš„æŸ¥è¯¢ï¼š

1. åŸºæœ¬æ¦‚å†µï¼š
   - [_å…·ä½“åº”æ€¥äº‹ä»¶åç§°_] å‘ç”Ÿæ—¶é—´å’Œåœ°ç‚¹
   - [_å…·ä½“åº”æ€¥äº‹ä»¶åç§°_] ç›´æ¥åŸå› å’Œåˆæ­¥å½±å“
   - [_å…·ä½“åº”æ€¥äº‹ä»¶åç§°_] åˆå§‹å…¬å…±ååº”å’Œåª’ä½“æŠ¥é“

2. å¤„ç½®è¿‡ç¨‹ï¼š
   - [_å…·ä½“åº”æ€¥äº‹ä»¶åç§°_] åº”æ€¥æªæ–½å’Œæ—¶é—´çº¿
   - [_å…·ä½“åº”æ€¥äº‹ä»¶åç§°_] æ”¿åºœå’Œéæ”¿åºœç»„ç»‡è§’è‰²
   - [_å…·ä½“åº”æ€¥äº‹ä»¶åç§°_] ç¾å®³å“åº”ç‰©èµ„å’Œäººå‘˜éƒ¨ç½²

3. å†›æ°‘åä½œï¼š
   - [_å…·ä½“åº”æ€¥äº‹ä»¶åç§°_] å†›é˜Ÿæ•‘æ´è¡ŒåŠ¨å’Œåä½œç»†èŠ‚
   - [_å…·ä½“åº”æ€¥äº‹ä»¶åç§°_] å†›é˜Ÿä¸åœ°æ–¹æ”¿åºœåˆä½œæœºåˆ¶
   - [_å…·ä½“åº”æ€¥äº‹ä»¶åç§°_] å†›æ°‘åˆä½œä¸­çš„æŒ‘æˆ˜å’Œå…‹æœ

4. å½±å“è¯„ä¼°ï¼š
   - [_å…·ä½“åº”æ€¥äº‹ä»¶åç§°_] ç¾å®³å¯¹ç¯å¢ƒçš„é•¿æœŸå½±å“
   - [_å…·ä½“åº”æ€¥äº‹ä»¶åç§°_] ç¾å®³å¯¹ç»æµå’Œç¤¾ä¼šçš„å½±å“
   - [_å…·ä½“åº”æ€¥äº‹ä»¶åç§°_] å—ç¾ç¾¤ä½“å’Œåœ°åŒºçš„æ¢å¤è¿›ç¨‹

5. åæ€å’Œå¯ç¤ºï¼š
   - [_å…·ä½“åº”æ€¥äº‹ä»¶åç§°_] ç¾å®³ç®¡ç†å’Œåº”å¯¹çš„æœ‰æ•ˆæ€§è¯„ä¼°
   - [_å…·ä½“åº”æ€¥äº‹ä»¶åç§°_] ä»ç¾å®³ä¸­å­¦åˆ°çš„æ•™è®­å’Œæ”¹è¿›æªæ–½
   - [_å…·ä½“åº”æ€¥äº‹ä»¶åç§°_] å¯¹æœªæ¥ç¾å®³åº”æ€¥å‡†å¤‡çš„å»ºè®®å’Œç­–ç•¥

è¯·ç¡®ä¿æ›¿æ¢[_å…·ä½“åº”æ€¥äº‹ä»¶åç§°_]ä¸ºä½ æ­£åœ¨ç ”ç©¶çš„äº‹ä»¶åç§°ï¼Œä»¥å®šä½å‡†ç¡®ç›¸å…³çš„èµ„æ–™ã€‚æ ¹æ®æœç´¢ç»“æœçš„è¯¦å°½ç¨‹åº¦ï¼Œé€‚æ—¶è°ƒæ•´æˆ–ç»†åŒ–ä½ çš„æŸ¥è¯¢å…³é”®è¯ã€‚
[_å…·ä½“åº”æ€¥äº‹ä»¶åç§°_]: {_context}
{_task}:
"""

_gec_ask = """é¢å¯¹'{_context}'çš„ä¸¥å³»æŒ‘æˆ˜ï¼Œå›½å®¶å’Œåœ°æ–¹åº”æ€¥ç®¡ç†éƒ¨é—¨éœ€è¿…é€ŸæŒæ¡å…¨é¢è€Œè¯¦å°½çš„ä¿¡æ¯ï¼Œä»¥ä¾¿å½¢æˆæœ‰æ•ˆçš„åº”å¯¹ç­–ç•¥ã€‚ç°åœ¨ï¼Œè¯·ä½ æ ¹æ®ä»¥ä¸‹é˜¶æ®µæ€§ä¿¡æ¯éœ€æ±‚ï¼Œç”Ÿæˆä¸€ç³»åˆ—çš„æŸ¥è¯¢é—®é¢˜ï¼Œä»¥å¸®åŠ©æˆ‘ä»¬æœé›†æœ‰å…³è¯¥äº‹ä»¶å…¨ç¨‹çš„å…³é”®ä¿¡æ¯ï¼š

1. ç¾å®³å‘ç”Ÿä¸åˆæœŸå“åº”ï¼š
   - è¯¥è‡ªç„¶ç¾å®³ç¡®åˆ‡çš„å‘ç”Ÿæ—¶é—´å’Œåœ°ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ
   - ç¾å®³çš„å¼ºåº¦å’Œè§„æ¨¡ï¼Ÿ
   - ç¾å®³å‘ç”Ÿåï¼Œé¦–æ‰¹é‡‡å–çš„åº”æ€¥å“åº”æªæ–½åŒ…æ‹¬å“ªäº›ï¼Ÿ

2. åº”æ€¥ååº”ä¸å¤„ç½®ç»è¿‡ï¼š
   - ç¾å®³å‘å±•æ‰©æ•£çš„è¿‡ç¨‹æ˜¯æ€æ ·çš„ï¼Ÿ
   - ç¾å®³å‘å±•æ‰©æ•£çš„æ—¶é—´çº¿å¦‚ä½•ï¼Ÿ
   - å®æ–½çš„åº”æ€¥å“åº”å’Œæ•‘æ´æªæ–½å…·ä½“åŒ…æ‹¬å“ªäº›ï¼Ÿ
   - å®æ–½åº”æ€¥å“åº”å’Œæ•‘æ´æªæ–½çš„æ—¶é—´çº¿å¦‚ä½•ï¼Ÿ
   - ç¾å®³çš„å…³é”®è½¬æŠ˜ç‚¹æœ‰å“ªäº›ï¼Ÿ
   - åœ¨ç¾å®³åº”å¯¹ä¸­ï¼Œæ¶‰åŠçš„å…³é”®äººç‰©å’Œç»„ç»‡æ‰®æ¼”äº†ä½•ç§è§’è‰²ï¼Ÿ
   - å“åº”è¿‡ç¨‹ä¸­ä¾æ®äº†å“ªäº›æ”¿ç­–æ³•è§„æ¥æŒ‡å¯¼è¡ŒåŠ¨ï¼Ÿ
   - ç¾å®³å¯¹ç¯å¢ƒã€å…¬å…±å¥åº·ã€ç»æµä»¥åŠç¤¾ä¼šç§©åºäº§ç”Ÿäº†å“ªäº›æ·±è¿œå½±å“ï¼Ÿ

3. ç¾æƒ…æ§åˆ¶ä¸è¿‡æ¸¡æ€§æªæ–½ï¼š
   - ç¾æƒ…å¾—åˆ°æ§åˆ¶çš„æ—¶é—´ç‚¹åŠå…¶æ ‡å¿—æ€§äº‹ä»¶æ˜¯ä»€ä¹ˆï¼Ÿ
   - æ§åˆ¶ç¾æƒ…åé‡‡å–äº†å“ªäº›å…³é”®æªæ–½å’Œç­–ç•¥ï¼Ÿ
   - é’ˆå¯¹ç¾å®³å—å½±å“äººç¾¤å®æ–½äº†å“ªäº›è¿‡æ¸¡æ€§æ•‘åŠ©å’Œæ”¯æŒï¼Ÿ

4. æ¢å¤é‡å»ºä¸æ³•è§„æ”¿ç­–å®æ–½ï¼š
   - ç¾åæ¢å¤å’Œé‡å»ºå·¥ä½œçš„èµ·å§‹æ—¶é—´å’Œé˜¶æ®µæ€§ç›®æ ‡æ˜¯ä»€ä¹ˆï¼Ÿ
   - é‡å»ºæœŸé—´ï¼Œåˆ¶å®šæˆ–ä¿®æ”¹äº†å“ªäº›ç›¸å…³æ³•å¾‹æ³•è§„ï¼Ÿ
   - æ¢å¤å’Œé‡å»ºæªæ–½çš„é•¿æœŸç¤¾ä¼šè¯„ä»·å’Œç”Ÿæ€å½±å“å¦‚ä½•ï¼Ÿ

5. æ•´ä½“è¯„ä¼°ä¸æ•™è®­å¸å–ï¼š
   - å¦‚ä½•ç³»ç»Ÿè¯„ä»·æœ¬æ¬¡è‡ªç„¶ç¾å®³çš„å¤„ç½®æ•ˆæœå’Œåº”æ€¥ç®¡ç†èƒ½åŠ›ï¼Ÿ
   - è¿™æ¬¡äº‹ä»¶å¯¹æœªæ¥ç¾å®³é£é™©è¯„ä¼°å’Œé¢„é˜²è®¡åˆ’æœ‰å“ªäº›å¯ç¤ºï¼Ÿ
   - ä¸ºæé«˜æœªæ¥åº”æ€¥å“åº”å’Œç¾å®³ç®¡ç†èƒ½åŠ›ï¼Œæå‡ºå“ªäº›å…·ä½“çš„å»ºè®®å’Œç­–ç•¥ï¼Ÿ

é€šè¿‡å›ç­”è¿™äº›è¯¦ç»†çš„é—®é¢˜ï¼Œæˆ‘ä»¬å¸Œæœ›èƒ½å¤Ÿå»ºç«‹ä¸€ä¸ªå…³äº'{_context}'çš„å…¨æ–¹ä½ã€æ·±å…¥çš„äº‹ä»¶æŠ¥å‘Šã€‚è¯·ä¸ºæ¯ä¸ªä¿¡æ¯ç‚¹ç”Ÿæˆå…·ä½“çš„æŸ¥è¯¢é—®é¢˜ï¼Œä»¥ä¾¿æˆ‘ä»¬å¯¹çŸ¥è¯†åº“è¿›è¡Œè¯¦ç»†çš„äº‹å®æœç´¢å’Œæ•°æ®åˆ†æã€‚
{_task}:
"""

# def do_multi_search(msg):
#     _agents = [
#     ]
#     _out = str(WORKSPACE_AUTOGEN / "organizer_output.txt")
#     def validate_results_func():
#         with open(_out, "r") as f:
#             content = f.read()
#         return bool(content)
#     _organizer = Organizer(
#         name="Search Team",
#         agents=_agents,
#         validate_results_func=validate_results_func,
#     )
#     _organizer_conversation_result = _organizer.broadcast_conversation(msg)
#     match _organizer_conversation_result:
#         case ConversationResult(success=True, cost=_cost, tokens=_tokens):
#             print(f"âœ… Organizer.Broadcast was successful. Team: {_organizer.name}")
#             print(f"ğŸ“Š Name: {_organizer.name} Cost: {_cost}, tokens: {_tokens}")
#             with open(_out, "r") as f:
#                 content = f.read()
#             return content
#         case _:
#             print(f"âŒ Organizer.Broadcast failed. Team: {_organizer.name}")

